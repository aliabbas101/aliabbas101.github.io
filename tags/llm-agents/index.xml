<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM Agents on Syed Ali Rizvi</title>
    <link>https://syedalirizvi.com/tags/llm-agents/</link>
    <description>Recent content in LLM Agents on Syed Ali Rizvi</description>
    <image>
      <title>Syed Ali Rizvi</title>
      <url>https://syedalirizvi.com/images/msg.png</url>
      <link>https://syedalirizvi.com/images/msg.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 24 Feb 2024 20:13:06 +0000</lastBuildDate>
    <atom:link href="https://syedalirizvi.com/tags/llm-agents/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Agents: Future of Generative Artificial Intelligence</title>
      <link>https://syedalirizvi.com/posts/llm-agents/</link>
      <pubDate>Sat, 24 Feb 2024 20:13:06 +0000</pubDate>
      <guid>https://syedalirizvi.com/posts/llm-agents/</guid>
      <description>Recently, State-of-the-Art (SOTA) language models such as OpenAI&amp;rsquo;s gpt-3.5-turbo, gpt-4, llama, Mixture of Experts models such as Mixtral&amp;rsquo;s mistral-8x7b have demonstrated exceptional capabilities when it comes tasks like code synthesis, Natural Language Understanding (NLU) and Natural Language Generation (NLG), the main reason being the volume of data they have been trained on, we have been witnessing a sudden technological shift and adaptation of these models and without ignoring the fact that if you have been active consumer of these models, either as an end-user or a developer, you might have already noticed some of their limitations as well.</description>
    </item>
  </channel>
</rss>
