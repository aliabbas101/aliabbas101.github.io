[{"content":"Large Language Models have recently demonstrated impressive conversational capabilities, the newer variants of Generative Pretrained Transformers such as GPT-4o from OpenAI is has been optimized for conversations,\nCore Components ","permalink":"https://syedalirizvi.com/posts/towards-low-latency-ai/","summary":"Large Language Models have recently demonstrated impressive conversational capabilities, the newer variants of Generative Pretrained Transformers such as GPT-4o from OpenAI is has been optimized for conversations,\nCore Components ","title":"Conversational AI: Building Low-Latency voicebots"},{"content":"Recently, State-of-the-Art (SOTA) language models such as OpenAI\u0026rsquo;s gpt-3.5-turbo, gpt-4, llama, Mixture of Experts models such as Mixtral\u0026rsquo;s mistral-8x7b have demonstrated exceptional capabilities when it comes tasks like code synthesis, Natural Language Understanding (NLU) and Natural Language Generation (NLG), the main reason being the volume of data they have been trained on, we have been witnessing a sudden technological shift and adaptation of these models and without ignoring the fact that if you have been active consumer of these models, either as an end-user or a developer, you might have already noticed some of their limitations as well.\nLet\u0026rsquo;s face it, Large Language Models (LLMs) cannot be relied upon in isolation for building robust AI applications, which has lead many developers ending up utilizing them only as core controllers sitting behind structured pipelines implemented as wrappers for these models for them to effectively generate relevant content.\nIn this article, I will be covering the main limitations of standalone Language Models and the modern techniques which have emerged lately to mitigate these issues.\nLimitations Alright, so what do we know about large language models so far ? They are great at generating content, from code snippets to textual data, but as\nNow based on the above findings, we know langauge models although demonstrating exceptional capabilities are still \u0026ldquo;limited\u0026rdquo;, in terms of:\nContextual Memory Planning Performing Actions. Access to External knowledge. LLM Agents: The modern approach Here\u0026rsquo;s a sample prompt:\nYou are an \u0026lt;Role\u0026gt; Think step by step before answering Memory Planning Actions Knowledge Conclusion References ","permalink":"https://syedalirizvi.com/posts/llm-agents/","summary":"Recently, State-of-the-Art (SOTA) language models such as OpenAI\u0026rsquo;s gpt-3.5-turbo, gpt-4, llama, Mixture of Experts models such as Mixtral\u0026rsquo;s mistral-8x7b have demonstrated exceptional capabilities when it comes tasks like code synthesis, Natural Language Understanding (NLU) and Natural Language Generation (NLG), the main reason being the volume of data they have been trained on, we have been witnessing a sudden technological shift and adaptation of these models and without ignoring the fact that if you have been active consumer of these models, either as an end-user or a developer, you might have already noticed some of their limitations as well.","title":"LLM Agents: Future of Generative Artificial Intelligence"},{"content":"","permalink":"https://syedalirizvi.com/experiments/","summary":"","title":"Experiments"}]